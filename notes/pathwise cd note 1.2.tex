\documentclass[12pt]{article}
\usepackage{anysize}
\marginsize{1.2cm}{1.4cm}{.4cm}{1cm}

\usepackage[normalem]{ulem}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\setlength{\parindent}{0pt}

\newcommand{\by}{\mathbf{y}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\btheta}{\mathbf{\theta}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bW}{\mathbf{W}}

\newcommand{\thh}{\hat{\theta}}
\newcommand{\sgn}{\text{ sign}}

\begin{document}

Ian Dardik \\
CS6140 \\
PCD for LASSO and Elastic Net

\section{Pathwise Coordinate Descent}
Pathwise coordinate descent is an effective algorithm for implementing regression algorithms such as LASSO and Elastic Net\cite{ht}.  Pathwise coordinate descent works by holding tuning parameters ($\lambda, \alpha$) constant and optimizing each parameter individually\cite{ht}.  Given data $\bX$, response variable $\bY$, and tuning parameters $\lambda$ and $\alpha$, we can find the solution to an optimization problem
	$$\mathbf{\thh} = \underset{\theta}{\text{argmin}}\{O(\theta,X,Y,\lambda,\alpha)\}$$
by considering each parameter $j$ as an individual optimization problem where we hold all other parameters $k \ne j$ constant.  We can find $\thh_j$ by finding the stationary points of a convex $O$ with respect to $\theta_j$:
	$$\thh_j = \theta_j \text{ s.t. } \frac{\partial O}{\partial \theta_j} = 0$$

In Pathwise Coordinate Descent we cycle through each parameter and make the appropriate update.  After each complete cycle, we check to see if the difference between the previous and the newly updated parameters is smaller than a given tolerance, and terminate if so.  


\section{Parameterizing the PCD Algorithm}
Section 1 shows that Pathwise Coordinate Descent is parameterized by the optimization problem $O$ that it solves for each $\theta_j$.  However, for the objective functions we consider in our project, we can parameterize the PCD algorithm by just two nonnegative parameters we call $A$ and $B$.  For each objective function we consider below, we will show that the solution to optimizing each $\theta_j$ takes the form:
	$$\theta_j = S\left(\sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k), B\right)/A$$

Where we define\cite{ht} $S(t,B) = \sgn(t)(|t|-B)_+$, and $()_+$ is the soft-thresholding operator.  It follows that each algorithm reduces to the following parameterized form: $\text{pathwise\_cd}(A, B, X, Y)$.  


\section{Standardized Data}
We will consider the LASSO and the Elastic Net in the following sections.  In both regression problems we will assume that our data is standardized so that each data vector in our design matrix has mean 0 and variance 1.  Thus we can omit the intercept term as well in our regression, since the solution will be equal to the mean\cite{ht}.  


\section{LASSO}
LASSO regression solves the following optimization problem for each $\theta_j$\cite{ht}:
	$$\thh_j = \underset{\theta}{\text{argmin}}\left\{ \frac{1}{2} \sum\limits_{i=1}^N \left(Y_i - \sum\limits_{k=1}^px_{ik}\theta_k \right)^2 + \lambda \sum\limits_{k=1}^p|\theta_k| \right\}$$

Where $\lambda \geq 0$ is a tuning parameter.  Let $O$ be the formula we are optimizing above.  We can solve for $\theta_j$ by finding the stationary points of $O$:
\begin{equation}\begin{split}
	\frac{\partial O}{\partial \theta_j}
		& = -\sum\limits_{i=1}^N X_{ij}(Y_i - X_{ij}\theta_j - \sum\limits_{k \ne j}^p X_{ik}\theta_k) + \lambda \sgn(\theta_j) \\
		& = -\sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k) + \sum\limits_{i=1}^NX_{ij}^2\theta_j + \lambda \sgn(\theta_j) \\
		& = -\sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k) +(N-1)\theta_j + \lambda \sgn(\theta_j) \\
\end{split}\end{equation}

The last step follows from the fact that our data is standardized, and thus $\sum\limits_{i=1}^NX_{ij}^2 = N-1$.  \\

Set $\frac{\partial O}{\partial \theta_j}=0$ and, for convenience, let $Q = \sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k)$ and and we get:
	$$(N-1)\theta_j + \lambda\sgn(\theta_j) = Q$$

To solve for $\theta_j$ notice that when $\theta_j \geq 0$, $\theta_j=\frac{Q-\lambda}{N-1}$ so $Q \geq \lambda$.  However when $\theta_j \leq 0$, $\theta_j=\frac{Q+\lambda}{N-1}$ so $Q \leq -\lambda$.  Notice the following two properties: \\
	\hspace*{1cm} (1) $|Q| \geq \lambda$ \\
	\hspace*{1cm} (2) $\theta_j \geq 0$ iff $Q \geq 0$ and $\theta_j \leq 0$ iff $Q \leq 0$ \\

Therefore, we can write the solution for $\theta_j$ as:
	$$\theta_j = \frac{\sgn(Q)(|Q|-\lambda)_+}{N-1}$$

Where the soft-thresholding operator guarantees property (1), while multiplying by $\sgn(Q)$ makes use of (and guarantees) property (2).  Recall from Section 2 that $S(t,B) = \sgn(t)(|t|-B)_+$, then our final solution is:
	$$\theta_j = S\left(\sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k), \lambda\right)/(N-1)$$

Notice this yields a solution nearly identical to Elements of Statistical Learning\cite{ht}; the difference is that we divide by $N-1$ and I'm not sure why they omit this in the book.  \\

Finally, it is clear that we can parameterize pathwise\_cd for the LASSO if we let $A=N-1$ and $B=\lambda$.  


\section{Elastic Net}
The Elastic Net uses the tuning parameter $\alpha \in [0,1]$ to compromise between Ridge and Lasso penalization.  It solves the following optimization problem:
	$$\thh_j = \underset{\theta}{\text{argmin}}\left\{ \frac{1}{2} \sum\limits_{i=1}^N \left(Y_i - \sum\limits_{k=1}^px_{ik}\theta_k \right)^2 + \lambda \sum\limits_{k=1}^p \left(\alpha\theta_k^2 + (1-\alpha)|\theta_k| \right) \right\}$$

This problem is very similar to LASSO above so I will write slightly more concisely here.  Let $O$ be the formula we are optimizing above.  We can solve for $\theta_j$ by finding the stationary points of $O$:
\begin{equation}\begin{split}
	\frac{\partial O}{\partial \theta_j}
		& = -\sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k) +(N-1)\theta_j + 2\lambda\alpha\theta_j + \lambda(1-\alpha)\sgn(\theta_j) \\
\end{split}\end{equation}

Set $\frac{\partial O}{\partial \theta_j}=0$ and, for convenience, let $Q = \sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k)$, $A=(N-1)+2\lambda\alpha$, and $B=\lambda(1-\alpha)$ and and we get:
	$$A\theta_j + B\sgn(\theta_j) = Q$$

Notice that since $A \geq 0$ and $B \geq 0$, we can use the same two case logic from the LASSO section, yielding the solution:
	$$\theta_j = \frac{\sgn(Q)(|Q|-B)_+}{A}$$

Our final solution is:
	$$\theta_j = S\left(\sum\limits_{i=1}^N X_{ij}(Y_i - \sum\limits_{k \ne j}^p X_{ik}\theta_k), B\right)/A$$

This is already in our desired form, and it is clear that we can parameterize pathwise\_cd for the Elastic Net for our choices of $A=(N-1)+2\lambda\alpha$ and $B=\lambda(1-\alpha)$.  

\begin{thebibliography}{9}
\bibitem{ht} 
Trevor Hastie, Robert Tibshirani, and Jerome Friedman,
The Elements of Statistical Learning 2nd Edition
\end{thebibliography}

\end{document}














